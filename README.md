# üåê AI Translator Agent

Um agente de tradu√ß√£o inteligente com **sistema h√≠brido de IA**: usa a API Groq (r√°pida e gratuita) como principal e faz fallback autom√°tico para modelo local na GPU quando o limite di√°rio √© atingido.

## ‚ú® Funcionalidades

- **üîÑ Sistema H√≠brido**: Groq como principal, Ollama (GPU local) como fallback autom√°tico
- **Tradu√ß√£o Multi-idioma**: Recebe texto em qualquer idioma e traduz para PT-BR, ES e EN
- **Corre√ß√£o Gramatical**: Corrige automaticamente erros gramaticais e melhora a sintaxe
- **Melhoria de Clareza**: Aprimora a fluidez e clareza do texto mantendo o sentido original
- **Detec√ß√£o de E-mails**: Identifica automaticamente e-mails e formata com sauda√ß√£o, corpo e despedida
- **Interface Web Moderna**: Interface intuitiva constru√≠da com Streamlit
- **Hist√≥rico de Conversas**: Mant√©m o contexto das tradu√ß√µes anteriores na sess√£o

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Usu√°rio                          ‚îÇ
‚îÇ                       ‚îÇ                             ‚îÇ
‚îÇ                       ‚ñº                             ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ              ‚îÇ   Streamlit     ‚îÇ                    ‚îÇ
‚îÇ              ‚îÇ   (Interface)   ‚îÇ                    ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                       ‚îÇ                             ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ         ‚ñº                           ‚ñº               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ   Groq    ‚îÇ  ‚îÄ‚îÄfallback‚îÄ‚îÄ‚ñ∂‚îÇ  Ollama   ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  (Cloud)  ‚îÇ              ‚îÇ  (Local)  ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ           ‚îÇ              ‚îÇ   GPU     ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ    Principal                   Backup               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üñ•Ô∏è Requisitos de Hardware

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| GPU NVIDIA | 8GB VRAM | 12GB+ VRAM |
| RAM | 16GB | 32GB |
| CPU | Qualquer x64 | Ryzen 5000+ / Intel 10th+ |

> ‚úÖ **Testado com**: RTX 3060 12GB, 32GB RAM, Ryzen 5800X

## üöÄ Pr√©-requisitos de Software

1. **Docker** e **Docker Compose** instalados
2. **NVIDIA Driver** atualizado (vers√£o 525+)
3. **NVIDIA Container Toolkit** instalado
4. **Chave de API da Groq** (gratuita): [console.groq.com/keys](https://console.groq.com/keys)

### Instalar NVIDIA Container Toolkit (se necess√°rio)

```bash
# Adicionar reposit√≥rio NVIDIA
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# Instalar
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Configurar Docker
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verificar instala√ß√£o
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
```

## üì¶ Instala√ß√£o

1. **Clone ou navegue at√© o diret√≥rio do projeto**:
```bash
cd ai-translator-agent
```

2. **Crie o arquivo `.env`** com sua chave da Groq:
```bash
echo 'GROQ_API_KEY=sua_chave_aqui' > .env
```

   > üìù Obtenha sua chave gratuita em: [console.groq.com/keys](https://console.groq.com/keys)

3. **Inicie os containers**:
```bash
docker compose up -d --build
```

4. **Baixe o modelo de fallback** (primeira execu√ß√£o):
```bash
docker exec -it ollama ollama pull qwen2.5:7b-instruct
```

5. **Acesse a aplica√ß√£o**:
   - Abra: `http://localhost:5000`
   - Se estiver no WSL: use o IP do WSL (`hostname -I`)

## üèÉ Como Usar

### Comandos Principais

```bash
# Iniciar a aplica√ß√£o
docker compose up -d

# Ver logs da aplica√ß√£o
docker compose logs -f ai-translator

# Ver logs do Ollama
docker compose logs -f ollama

# Parar tudo
docker compose down

# Reiniciar
docker compose restart

# Reconstruir ap√≥s mudan√ßas
docker compose up -d --build
```

### Verificar Status

```bash
# Status dos containers
docker compose ps

# Uso da GPU
nvidia-smi

# Modelos carregados no Ollama
docker exec -it ollama ollama list
```

## üìÅ Estrutura do Projeto

```
ai-translator-agent/
‚îú‚îÄ‚îÄ agent.py              # Aplica√ß√£o principal (Groq + Ollama)
‚îú‚îÄ‚îÄ Dockerfile            # Configura√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ docker-compose.yml    # Orquestra√ß√£o (app + Ollama)
‚îú‚îÄ‚îÄ requirements.txt      # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente (criar manualmente)
‚îî‚îÄ‚îÄ README.md             # Esta documenta√ß√£o
```

## üîß Tecnologias Utilizadas

- **Python 3.11**: Linguagem de programa√ß√£o
- **Streamlit 1.40.0**: Framework para interface web
- **Groq API**: Backend principal (cloud, r√°pido, limites di√°rios)
- **Ollama**: Backend de fallback (local, GPU)
- **Qwen2.5 7B**: Modelo local para tradu√ß√£o
- **Llama 3.3 70B**: Modelo cloud via Groq
- **Docker**: Containeriza√ß√£o
- **NVIDIA CUDA**: Acelera√ß√£o por GPU

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Modelos Dispon√≠veis

**Groq (cloud)**:
| Modelo | Descri√ß√£o |
|--------|-----------|
| `llama-3.3-70b-versatile` | **Padr√£o** - Melhor qualidade |
| `llama-3.1-8b-instant` | Mais r√°pido, menos preciso |
| `mixtral-8x7b-32768` | Boa alternativa |

**Ollama (local)**:
| Modelo | VRAM | Uso |
|--------|------|-----|
| `qwen2.5:7b-instruct` | ~4.4GB | **Padr√£o** - Melhor para tradu√ß√£o |
| `qwen2.5:3b-instruct` | ~2GB | Mais leve |
| `llama3.2:8b-instruct` | ~4.5GB | Alternativa |

### Trocar Modelos

Edite as vari√°veis no `docker-compose.yml`:
```yaml
environment:
  - GROQ_MODEL=llama-3.3-70b-versatile  # Modelo Groq
  - OLLAMA_MODEL=qwen2.5:7b-instruct     # Modelo local
```

## üí° Exemplo de Uso

**Entrada:**
```
Hello, I want to send a email to my boss about the project delay.
```

**Sa√≠da:**
```
### Portugu√™s (Brasil)
Ol√°, gostaria de enviar um e-mail ao meu chefe sobre o atraso do projeto.

### Espa√±ol
Hola, me gustar√≠a enviar un correo electr√≥nico a mi jefe sobre el retraso del proyecto.

### English
Hello, I would like to send an email to my boss about the project delay.
```

## üêõ Troubleshooting

### Groq: "Rate limit exceeded"

**Isso √© normal!** Quando o limite di√°rio do Groq √© atingido, o sistema automaticamente usa o modelo local (Ollama).

Para evitar:
- Use menos requisi√ß√µes
- Espere o reset di√°rio (meia-noite UTC)
- O fallback para Ollama √© autom√°tico

### Ollama: "Modelo n√£o carregado"

```bash
# Baixar o modelo
docker exec -it ollama ollama pull qwen2.5:7b-instruct

# Verificar modelos
docker exec -it ollama ollama list
```

### Ollama: "Container unhealthy"

```bash
# Aguardar inicializa√ß√£o (pode levar 30-60s)
docker compose ps

# Verificar logs
docker compose logs ollama
```

### GPU n√£o detectada

```bash
# Verificar driver NVIDIA
nvidia-smi

# Reinstalar NVIDIA Container Toolkit (ver se√ß√£o de pr√©-requisitos)

# Reiniciar Docker
sudo systemctl restart docker
```

### Aplica√ß√£o n√£o abre (WSL)

```bash
# Descobrir IP do WSL
hostname -I

# Acessar pelo IP, ex: http://172.30.242.142:5000
```

## üìä Monitoramento

```bash
# GPU em tempo real
watch -n 1 nvidia-smi

# Logs em tempo real
docker compose logs -f

# Status dos containers
docker compose ps
```

## üîí Privacidade e Custos

| Backend | Privacidade | Custo | Limite |
|---------|-------------|-------|--------|
| Groq | Dados v√£o para cloud | Gratuito | ~14.400 req/dia |
| Ollama | 100% local | Gratuito | Ilimitado |

- ‚úÖ Quando Groq atinge o limite, usa automaticamente Ollama (100% local)
- ‚úÖ Seus dados s√≥ saem da m√°quina quando usando Groq

## üìÑ Licen√ßa

Este projeto √© de uso pessoal/educacional.

---

**Desenvolvido com ‚ù§Ô∏è usando Streamlit, Groq e Ollama**

*Sistema h√≠brido: velocidade do cloud + privacidade do local!* üöÄ
