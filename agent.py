import os
import json

import requests
import streamlit as st
from dotenv import load_dotenv
from groq import Groq


# Carrega vari√°veis de ambiente do arquivo .env (se existir)
load_dotenv()

# Configura√ß√£o do Groq (principal)
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")

# Configura√ß√£o do Ollama (fallback)
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct")


SYSTEM_PROMPT = """Voc√™ √© um assistente de tradu√ß√£o e revis√£o de texto.

REGRAS GERAIS:
1. Receba uma mensagem em qualquer idioma (pode ser uma frase completa ou uma palavra simples).
2. IMPORTANTE: Voc√™ DEVE SEMPRE traduzir o que foi enviado. NUNCA converse com o usu√°rio, n√£o fa√ßa perguntas, n√£o d√™ explica√ß√µes. Apenas traduza.
3. Se receber uma palavra simples, traduza a palavra simples.
4. Se receber uma frase, traduza a frase.
5. Sempre:
   - Corrija erros gramaticais (quando aplic√°vel).
   - Melhore a sintaxe, fluidez e clareza (quando aplic√°vel a frases).
   - Mantenha o sentido original da mensagem.
6. Gere SEMPRE as tr√™s vers√µes abaixo, todas j√° corrigidas e melhoradas:
   - Portugu√™s do Brasil.
   - Espanhol (neutro).
   - Ingl√™s (internacional).

FORMATO DA RESPOSTA:
Responda sempre em Markdown seguindo exatamente esta estrutura:

### Portugu√™s (Brasil)
<texto em portugu√™s formatado conforme as regras acima>

### Espa√±ol
<texto em espanhol formatado conforme as regras acima>

### English
<texto em ingl√™s formatado conforme as regras acima>

LEMBRE-SE: Voc√™ √© apenas um tradutor. Traduza sempre, sem conversar ou explicar.
"""


st.set_page_config(
    page_title="AI Translator Agent",
    page_icon="üåê",
    layout="wide",
    initial_sidebar_state="expanded",
)


# ============================================
# Fun√ß√µes para Groq
# ============================================

def init_groq_client():
    """Inicializa o cliente Groq se a API key estiver dispon√≠vel."""
    if not GROQ_API_KEY:
        return None, "API Key n√£o configurada"
    try:
        client = Groq(api_key=GROQ_API_KEY)
        return client, None
    except Exception as e:
        return None, str(e)


def call_groq(client, messages):
    """
    Chama a API da Groq.
    Retorna (resposta, is_rate_limited)
    """
    try:
        chat_completion = client.chat.completions.create(
            messages=messages,
            model=GROQ_MODEL,
            temperature=0.3,
            max_tokens=2048,
        )
        return chat_completion.choices[0].message.content, False
    except Exception as e:
        error_str = str(e).lower()
        # Verifica se √© erro de rate limit
        if "rate" in error_str or "limit" in error_str or "429" in error_str or "quota" in error_str:
            return None, True
        raise


# ============================================
# Fun√ß√µes para Ollama (fallback)
# ============================================

def check_ollama_status():
    """Verifica se o Ollama est√° dispon√≠vel e se o modelo est√° carregado."""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [m.get("name", "") for m in models]
            model_available = any(
                OLLAMA_MODEL in name or OLLAMA_MODEL.split(":")[0] in name
                for name in model_names
            )
            return True, model_available, model_names
        return False, False, []
    except requests.exceptions.RequestException:
        return False, False, []


def call_ollama(messages):
    """Chama o modelo via Ollama usando a API de chat."""
    url = f"{OLLAMA_BASE_URL}/api/chat"
    payload = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": False,
        "options": {
            "temperature": 0.3,
            "num_predict": 2048,
        },
    }

    try:
        response = requests.post(url, json=payload, timeout=300)
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"Erro ao conectar ao Ollama: {e}") from e

    if response.status_code != 200:
        raise RuntimeError(f"Ollama retornou c√≥digo {response.status_code}: {response.text}")

    try:
        data = response.json()
    except json.JSONDecodeError as e:
        raise RuntimeError(f"Resposta inv√°lida do Ollama: {e}") from e

    msg = data.get("message")
    if not msg or "content" not in msg:
        raise RuntimeError(f"N√£o foi poss√≠vel encontrar o conte√∫do na resposta: {data}")

    return msg["content"]


# ============================================
# Interface Streamlit
# ============================================

# Inicializa cliente Groq
groq_client, groq_error = init_groq_client()

# Verifica status do Ollama
ollama_online, ollama_model_ready, available_models = check_ollama_status()


with st.sidebar:
    st.title("üåê AI Translator Agent")
    st.markdown(
        """
        Envie uma frase ou texto em qualquer idioma.

        O agente vai:
        - **Corrigir gram√°tica e clareza**  
        - **Traduzir para PT-BR, Espanhol e Ingl√™s**
        """
    )

    st.markdown("---")
    st.subheader("üì° Status dos Backends")

    # Status do Groq
    if groq_client:
        st.success(f"‚òÅÔ∏è Groq: Online (`{GROQ_MODEL}`)")
    else:
        st.warning(f"‚òÅÔ∏è Groq: {groq_error or 'N√£o configurado'}")

    # Status do Ollama
    if ollama_online and ollama_model_ready:
        st.success(f"üñ•Ô∏è Ollama: Online (`{OLLAMA_MODEL}`)")
    elif ollama_online:
        st.warning(f"üñ•Ô∏è Ollama: Modelo n√£o carregado")
        st.caption(f"Execute: `docker exec -it ollama ollama pull {OLLAMA_MODEL}`")
    else:
        st.error("üñ•Ô∏è Ollama: Offline")

    st.markdown("---")
    st.caption("**Prioridade:** Groq ‚Üí Ollama (fallback)")
    st.caption("Se o limite di√°rio do Groq acabar, usa modelo local automaticamente.")


st.title("AI Translator Agent")
st.caption(
    "Cole uma frase em qualquer idioma. "
    "O agente vai revisar e traduzir para Portugu√™s (BR), Espanhol e Ingl√™s."
)


# Inicializa hist√≥rico
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe mensagens anteriores
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# Input do usu√°rio
user_input = st.chat_input("Digite o texto a ser traduzido (qualquer idioma)...")

if user_input:
    # Verifica se pelo menos um backend est√° dispon√≠vel
    if not groq_client and not (ollama_online and ollama_model_ready):
        st.error(
            "‚ùå Nenhum backend dispon√≠vel!\n\n"
            "- Configure `GROQ_API_KEY` no arquivo `.env`\n"
            "- Ou aguarde o Ollama inicializar e baixe o modelo"
        )
        st.stop()

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    messages_for_api = [{"role": "system", "content": SYSTEM_PROMPT}]
    for msg in st.session_state.messages:
        messages_for_api.append(msg)

    with st.chat_message("assistant"):
        translated_response = None
        backend_used = None

        # Tenta Groq primeiro
        if groq_client:
            with st.spinner("‚òÅÔ∏è Gerando tradu√ß√µes via Groq..."):
                try:
                    response, is_rate_limited = call_groq(groq_client, messages_for_api)
                    if response:
                        translated_response = response
                        backend_used = "groq"
                    elif is_rate_limited:
                        st.warning("‚ö†Ô∏è Limite di√°rio do Groq atingido. Usando modelo local...")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erro no Groq: {e}. Tentando modelo local...")

        # Fallback para Ollama se Groq falhou ou n√£o est√° dispon√≠vel
        if translated_response is None:
            if ollama_online and ollama_model_ready:
                with st.spinner("üñ•Ô∏è Gerando tradu√ß√µes via Ollama (GPU local)..."):
                    try:
                        translated_response = call_ollama(messages_for_api)
                        backend_used = "ollama"
                    except Exception as e:
                        st.error(f"‚ùå Erro ao usar Ollama: {e}")
            else:
                st.error(
                    "‚ùå N√£o foi poss√≠vel gerar a tradu√ß√£o.\n\n"
                    "- Groq atingiu o limite ou est√° indispon√≠vel\n"
                    "- Ollama n√£o est√° pronto como fallback"
                )

        # Exibe resposta se obtida
        if translated_response:
            # Indicador de qual backend foi usado
            if backend_used == "groq":
                st.caption("_‚òÅÔ∏è Resposta gerada via Groq_")
            else:
                st.caption("_üñ•Ô∏è Resposta gerada via Ollama (GPU local)_")

            st.markdown(translated_response)

            st.session_state.messages.append(
                {"role": "assistant", "content": translated_response}
            )
